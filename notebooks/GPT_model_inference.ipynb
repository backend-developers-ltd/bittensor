{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Model Inference\n",
    "\n",
    "Welcome! This notebook is a tutorial on how to use the model you've just trained on the Bittensor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bittensor\n",
    "from synapses.gpt2 import GPT2Synapse\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model\n",
    "You can find the model under `~/.bittensor/miners/gpt2-genesis/<miner_trial_id>/model.torch`. This is the default place that miners will store models, and your trial ID is auto-generated each time you run your miner, so if you don't know your trial ID you can always simply find the latest trial ID directory and use the `model.torch` there, as that will be your latest run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined loss (local, remote, and distilled) of preloaded model: 8.745189072843083:\n"
     ]
    }
   ],
   "source": [
    "model_path = '../../miners/gpt2-genesis/gpugpt04'\n",
    "\n",
    "# Check which device this machine is on, just in case we're not loading the model on the same machine that we trained it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(\"{}/model.torch\".format(model_path), map_location=device)\n",
    "\n",
    "\n",
    "# Let's load up a Bittensor config\n",
    "config = bittensor.neuron.Neuron.default_config()\n",
    "config = GPT2Synapse.default_config()\n",
    "\n",
    "# Let's load up the same synapse config we trained our model with\n",
    "config.synapse.n_head = 32\n",
    "config.synapse.n_layer = 12\n",
    "config.synapse.block_size = 20\n",
    "config.synapse.device = device\n",
    "\n",
    "# Load up the model\n",
    "model = GPT2Synapse(config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Combined loss (local, remote, and distilled) of preloaded model: {}:\".format(checkpoint['loss']))\n",
    "# Load up the huggingface tokenizer\n",
    "tokenizer = bittensor.__tokenizer__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference function\n",
    "In essence, the output of the current GPT model is simply encoded using the HuggingFace tokenizer that Bittensor uses. We need to simply decode that information out using the same tokenizer and turn it into text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()-1\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits = model.local_forward(x, training=False)\n",
    "        logits = model.target_layer(logits.local_hidden)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat  up broke the window.The mouse was out for the this cheese.Did\n"
     ]
    }
   ],
   "source": [
    "context = \"The cat \"\n",
    "\n",
    "# Tokenize the input\n",
    "x = tokenizer(context, padding=True, truncation=True)['input_ids']\n",
    "# Turn it into a tensor\n",
    "x = torch.tensor(x, dtype=torch.long)\n",
    "# Give it an extra dimension for the network's sake (expects a 2D tensor input)\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "# Let's sample the network for some output\n",
    "y = sample(model, x, 15, temperature=1.0, sample=True, top_k=10)\n",
    "\n",
    "# Decode the output\n",
    "completion = ''.join([tokenizer.decode(i, skip_special_tokens=True) for i in y])\n",
    "\n",
    "# Print what the model has predicted\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bittensor",
   "language": "python",
   "name": "bittensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
