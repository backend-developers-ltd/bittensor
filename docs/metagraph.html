<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <title>metagraph.py</title>
  <link rel="stylesheet" href="pycco.css">
</head>
<body>
<div id='container'>
  <div id="background"></div>
  <div class='section'>
    <div class='docs'><h1>metagraph.py</h1></div>
  </div>
  <div class='clearall'>
  <div class='section' id='section-0'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-0'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">bittensor</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">netaddr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="kn">from</span> <span class="nn">munch</span> <span class="kn">import</span> <span class="n">Munch</span>
<span class="kn">from</span> <span class="nn">loguru</span> <span class="kn">import</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">bittensor</span> <span class="kn">import</span> <span class="n">bittensor_pb2</span>
<span class="kn">from</span> <span class="nn">bittensor.subtensor</span> <span class="kn">import</span> <span class="n">WSClient</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">bittensor.exceptions.handlers</span> <span class="kn">import</span> <span class="n">rollbar</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-1'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-1'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre><span class="k">def</span> <span class="nf">int_to_ip</span><span class="p">(</span><span class="n">int_val</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">netaddr</span><span class="o">.</span><span class="n">IPAddress</span><span class="p">(</span><span class="n">int_val</span><span class="p">))</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-2'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-2'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre><span class="k">def</span> <span class="nf">ip_to_int</span><span class="p">(</span><span class="n">str_val</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">netaddr</span><span class="o">.</span><span class="n">IPAddress</span><span class="p">(</span><span class="n">str_val</span><span class="p">))</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-3'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-3'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre><span class="k">class</span> <span class="nc">ChainState</span><span class="p">():</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-4'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-4'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-5'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-5'>#</a>
      </div>
      <p>Cached values.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_uid</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stake</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lastemit</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neuron_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_pubkeys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_for_uid</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_for_pubkey</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pubkey_for_index</span> <span class="o">=</span> <span class="p">{}</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-6'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-6'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="k">def</span> <span class="nf">add_or_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pubkey</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">ip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">port</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lastemit</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">stake</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">w_keys</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">w_vals</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
        <span class="n">neuron</span> <span class="o">=</span> <span class="n">bittensor_pb2</span><span class="o">.</span><span class="n">Neuron</span><span class="p">(</span>
            <span class="n">version</span><span class="o">=</span><span class="n">bittensor</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
            <span class="n">public_key</span><span class="o">=</span><span class="n">pubkey</span><span class="p">,</span>
            <span class="n">address</span><span class="o">=</span><span class="n">int_to_ip</span><span class="p">(</span><span class="n">ip</span><span class="p">),</span>
            <span class="n">port</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">port</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">pubkey</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_for_pubkey</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_for_pubkey</span><span class="p">[</span><span class="n">pubkey</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">neuron</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stake</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">stake</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lastemit</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">lastemit</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_pubkeys</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">w_keys</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_vals</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">w_vals</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
            <span class="n">uid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_uid</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">next_uid</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index_for_pubkey</span><span class="p">[</span><span class="n">pubkey</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pubkey_for_index</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">pubkey</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">neuron</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stake</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">stake</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lastemit</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">lastemit</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_pubkeys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">w_keys</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">w_vals</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">uids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">uid</span> <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index_for_uid</span><span class="p">[</span><span class="n">uid</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-7'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-7'>#</a>
      </div>
      <p>Static network state object.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre><span class="k">class</span> <span class="nc">TorchChainState</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Maintains the chain state as a torch object.</span>
<span class="sd">        Params:</span>
<span class="sd">            tau: (int): </span>
<span class="sd">                current, per block, token inflation rate.</span>

<span class="sd">            block: (int):</span>
<span class="sd">                state block number.</span>

<span class="sd">            uids: (:obj:`torch.LongTensor` of shape :obj:`(n)`):</span>
<span class="sd">                UIDs for each neuron ordered by index.</span>
<span class="sd">            </span>
<span class="sd">            indices: (:obj:`torch.LongTensor` of shape :obj:`(n)`):</span>
<span class="sd">                Index of neurons, range(n)</span>

<span class="sd">            stake: (:obj:`torch.LongTensor` of shape :obj:`(n)`):</span>
<span class="sd">                Stake balance for each neuron ordered by index.</span>
<span class="sd">                </span>
<span class="sd">            lastemit: (:obj:`torch.LongTensor` of shape :obj:`(n)`):</span>
<span class="sd">                Last emission call for each neuron ordered by index.</span>

<span class="sd">            weights: (:obj:`torch.FloatTensor` of shape :obj:`(n)`):</span>
<span class="sd">                This neuron&#39;s weights W[,:]</span>

<span class="sd">            W: (:obj:`torch.FloatTensor` of shape :obj:`(n, n)`):</span>
<span class="sd">                Full weight matrix on chain.</span>

<span class="sd">            neurons: (List[bittensor_pb2.Neuron]) </span>
<span class="sd">                List of endpoints on the network.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        state = TorchChainState()</span>
<span class="sd">        state.n = cache.n</span>
<span class="sd">        state.tau = torch.tensor([50.0], dtype = torch.float32)</span>
<span class="sd">        state.neurons = copy.deepcopy(cache.neurons)</span>
<span class="sd">        state.indices = torch.tensor(range(state.n), dtype=torch.int64)</span>
<span class="sd">        state.uids = torch.tensor(copy.deepcopy(cache.uids), dtype=torch.int64)</span>
<span class="sd">        state.lastemit = torch.tensor(copy.deepcopy(cache.lastemit), dtype=torch.int64)</span>
<span class="sd">        state.stake = torch.tensor(copy.deepcopy(cache.stake), dtype=torch.float32)</span>
<span class="sd">        weights_numpy = numpy.zeros( (state.n, state.n) )</span>
<span class="sd">        for i in range(state.n):</span>
<span class="sd">            keys = cache.weight_pubkeys[i]</span>
<span class="sd">            vals = cache.weight_vals[i]</span>
<span class="sd">            val_sum = sum(vals)</span>
<span class="sd">            for k, val in list(zip(keys, vals)):</span>
<span class="sd">                if k in cache.index_for_pubkey:</span>
<span class="sd">                    j = cache.index_for_pubkey[k]</span>
<span class="sd">                    weights_numpy[i, j] = float(val) / float(val_sum)</span>
<span class="sd">        state.W = torch.tensor(weights_numpy, dtype=torch.float32)</span>
<span class="sd">        return state</span>
<span class="sd">#DIVIDER</span>
<span class="sd">class Metagraph():</span>
<span class="sd">#DIVIDER</span>
<span class="sd">    def __init__(self, config, keypair):</span>
<span class="sd">        r&quot;&quot;&quot;</span><span class="n">Initializes</span> <span class="n">a</span> <span class="n">new</span> <span class="n">Metagraph</span> <span class="n">subtensor</span> <span class="n">interface</span><span class="o">.</span>
        <span class="n">Args</span><span class="p">:</span>
            <span class="n">config</span> <span class="p">(</span><span class="n">bittensor</span><span class="o">.</span><span class="n">Config</span><span class="p">):</span>
                <span class="n">An</span> <span class="n">bittensor</span> <span class="n">config</span> <span class="nb">object</span><span class="o">.</span>
            <span class="n">keypair</span> <span class="p">(</span><span class="n">substrateinterface</span><span class="o">.</span><span class="n">Keypair</span><span class="p">):</span>
                <span class="n">An</span> <span class="n">bittensor</span> <span class="n">keys</span> <span class="nb">object</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-8'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-8'>#</a>
      </div>
      <p>def <strong>init</strong>(self):
    self.tau = torch.tensor([50.0], dtype = torch.float32)
    self.block = 0
    self.n = 0
    self.uids = torch.tensor([])
    self.indices = torch.tensor([])
    self.stake = torch.tensor([])
    self.lastemit = torch.tensor([])
    self.W = torch.tensor([[]])
    self.neurons = []</p>
<p>@staticmethod
def from_cache(cache: ChainState):
    r&rdquo;&ldquo;&rdquo; Deep copies from the chain state.</p>
<p>Deep copies chain state into metagraph state.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">n</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-9'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-9'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">block</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Return the block number when the chain state was updated.</span>
<span class="sd">        Returns</span>
<span class="sd">            block: (int):</span>
<span class="sd">                local chain state block number.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        return self.state.lastemit</span>
<span class="sd">#DIVIDER</span>
<span class="sd">    @property</span>
<span class="sd">    def indices(self) -&gt; torch.LongTensor:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Return</span> <span class="n">the</span> <span class="n">indices</span> <span class="n">of</span> <span class="n">each</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">chain</span> <span class="n">state</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span>
        <span class="n">Returns</span>
            <span class="n">indices</span><span class="p">:</span> <span class="p">(:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="err">`</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="err">`</span><span class="p">):</span>
                <span class="n">returned</span> <span class="n">indices</span> <span class="k">for</span> <span class="n">each</span> <span class="n">neuron</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-10'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-10'>#</a>
      </div>
      <h1>Protected vars</h1>
<p>self._config = config
self.__keypair = keypair</p>
<h1>Client for talking to chain.</h1>
<p>self.subtensor_client = WSClient(self._config.metagraph.chain_endpoint, self.__keypair)</p>
<h1>Chain state cached before converted into the torch state.</h1>
<p>self.cache = ChainState()</p>
<h1>Chain state as torch values.</h1>
<p>self.state = TorchChainState.from_cache(self.cache)</p>
<pre><code>@staticmethod   
def add_args(parser: argparse.ArgumentParser) -&gt; argparse.ArgumentParser:
</code></pre>
<h1>TODO(const): check this endpoint in check_config.</h1>
<p>parser.add_argument(&lsquo;&ndash;metagraph.chain_endpoint&rsquo;, default=&lsquo;206.189.254.5:12345&rsquo;, type=str, 
                    help=&rsquo;chain endpoint.&rsquo;)
parser.add_argument(&lsquo;&ndash;metagraph.stale_emit_filter&rsquo;, default=10000, type=int, 
                    help=&rsquo;filter neurons with last emit beyond this many blocks.&rsquo;)</p>
<p>return parser</p>
<pre><code>@staticmethod   
def check_config(config: Munch) -&gt; Munch:
</code></pre>
<p>return config</p>
<pre><code>@property
def n(self) -&gt; int:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Return the number of known neurons on chain.
Returns
    n: (int):
        number of known neurons.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">uids</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-11'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-11'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">stake</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Returns the stake held by each known neuron.</span>
<span class="sd">        Returns</span>
<span class="sd">            stake: (:obj:`torch.FloatTensor` of shape :obj:`(n)`):</span>
<span class="sd">                stake of each known neuron.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        return self.state.stake</span>
<span class="sd">#DIVIDER</span>
<span class="sd">    @property</span>
<span class="sd">    def tau(self) -&gt; torch.FloatTensor:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">tau</span><span class="p">:</span> <span class="n">the</span> <span class="n">chain</span> <span class="n">per</span> <span class="n">block</span> <span class="n">inflation</span> <span class="n">rate</span><span class="o">.</span> <span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span> <span class="mi">50</span>
        <span class="n">Returns</span>
            <span class="n">tau</span><span class="p">:</span> <span class="p">(:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="n">torchFloatTensor</span><span class="err">`</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="err">`</span><span class="p">):</span>
                <span class="n">current</span> <span class="n">chain</span> <span class="n">inflation</span> <span class="n">rate</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-12'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-12'>#</a>
      </div>
      <p>return self.state.block</p>
<pre><code>@property
def lastemit(self) -&gt; torch.LongTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns the last emit time for each known neuron.
Returns
    lastemit: (int):
        last emit time.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">I</span> <span class="o">=</span>  <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ranks</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ranks</span><span class="p">)</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">I</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">I</span><span class="p">),</span> <span class="n">I</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">I</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-13'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-13'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">I</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Returns the inflation incentive for each peer per block.</span>
<span class="sd">        Returns</span>
<span class="sd">            I: (:obj:`torch.FloatTensor` of shape :obj:`(n)`):</span>
<span class="sd">                stake of each known neuron.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        if self.W.shape[0] == 0:</span>
<span class="sd">            return torch.tensor([])</span>
<span class="sd">        else:</span>
<span class="sd">#DIVIDER</span>
<span class="sd">            S = self.S.view(self.state.n, 1)</span>
<span class="sd">            W = torch.transpose(self.W.view(self.state.n, self.state.n), 0, 1)</span>
<span class="sd">            R = torch.matmul(W, S).view(self.state.n)</span>
<span class="sd">        return R</span>
<span class="sd">#DIVIDER</span>
<span class="sd">    @property</span>
<span class="sd">    def R(self) -&gt; torch.FloatTensor:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Returns</span> <span class="n">ranks</span> <span class="k">for</span> <span class="n">each</span> <span class="n">known</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">graph</span><span class="o">.</span>
        <span class="n">Returns</span>
            <span class="n">rank</span><span class="p">:</span> <span class="p">(:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="err">`</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="err">`</span><span class="p">):</span>
                <span class="n">rank</span> <span class="n">of</span> <span class="n">each</span> <span class="n">known</span> <span class="n">neuron</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-14'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-14'>#</a>
      </div>
      <p>return self.state.indices</p>
<pre><code>@property
def uids(self) -&gt; torch.LongTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns unique ids for each neuron in the chain state.
Returns
    uids: (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(n)</code>):
        unique id for each neuron.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">W</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-15'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-15'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">bittensor_pb2</span><span class="o">.</span><span class="n">Neuron</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Return neuron endpoint information for each neuron.</span>
<span class="sd">        Returns</span>
<span class="sd">            neurons: (:obj:`List[bittensor_pb2.Neuron]` of shape :obj:`(n, n)`):</span>
<span class="sd">                endpoint information for each neuron.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        return [n.public_key for n in self.state.neurons]</span>
<span class="sd">#DIVIDER</span>
<span class="sd">    @property</span>
<span class="sd">    def weights(self) -&gt; torch.FloatTensor:</span>
<span class="sd">        r&quot;&quot;&quot;</span><span class="n">Return</span> <span class="n">this</span> <span class="n">neuron</span><span class="s1">&#39;s weights. W[0,:]</span>
        <span class="n">Returns</span> 
            <span class="n">weights</span><span class="p">:</span> <span class="p">(:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="err">`</span> <span class="n">of</span> <span class="n">shape</span> <span class="p">:</span><span class="n">obj</span><span class="p">:</span><span class="err">`</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="err">`</span><span class="p">):</span>
                <span class="n">returned</span> <span class="n">indices</span> <span class="k">for</span> <span class="n">passed</span> <span class="n">uids</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-16'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-16'>#</a>
      </div>
      <p>return self.state.stake</p>
<pre><code>@property
def S(self) -&gt; torch.FloatTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns the stake held by each known neuron.
Returns
    S: (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(n)</code>):
        stake of each known neuron.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">uids</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">uids</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">uids</span><span class="p">)</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Passed uids are not a subset of class.uids, with passed: </span><span class="si">{}</span><span class="s1"> and class.uids: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">uids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">uids</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">indices</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-17'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-17'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>    <span class="k">def</span> <span class="nf">uids_to_neurons</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">uids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">bittensor_pb2</span><span class="o">.</span><span class="n">Neuron</span><span class="p">]:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Returns a list with neurons for each uid.</span>
<span class="sd">        Args:</span>
<span class="sd">            uids: (torch.LongTensor)</span>
<span class="sd">                uids into neurons protos</span>
<span class="sd">        Returns:</span>
<span class="sd">            neurons: (List[bittensor_pb2.Neuron]): </span>
<span class="sd">                neuron info ordered by passed uids.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        loop = asyncio.get_event_loop()</span>
<span class="sd">        return loop.run_until_complete(self.async_chain_weights())</span>

<span class="sd">    async def async_chain_weights(self) -&gt; torch.FloatTensor:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Async</span><span class="p">:</span> <span class="n">returns</span> <span class="n">your</span> <span class="n">current</span> <span class="n">weights</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">chain</span><span class="o">.</span>
        <span class="n">Returns</span><span class="p">:</span>
            <span class="n">weights</span><span class="p">:</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span> <span class="n">weights</span> <span class="n">on</span> <span class="n">chain</span> <span class="k">for</span> <span class="n">each</span> <span class="n">neuron</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-18'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-18'>#</a>
      </div>
      <p>return self.state.tau</p>
<pre><code>@property
def incentive(self) -&gt; torch.FloatTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns the ranks 
Returns
    incentive: (:obj:<code>torch.FLoatTensor</code> of shape :obj:<code>(n)</code>):
        inflation incentive of each each known neuron.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">set_debug</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loop</span><span class="o">.</span><span class="n">run_until_complete</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">async_block</span><span class="p">())</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">async_block</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Async returns the current block on the chain.</span>
<span class="sd">        Returns:</span>
<span class="sd">            block: (int) block number on chain.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        loop = asyncio.get_event_loop()</span>
<span class="sd">        loop.set_debug(enabled=True)</span>
<span class="sd">        return loop.run_until_complete(self.async_subscribe(timeout))</span>

<span class="sd">    async def async_subscribe (self, timeout) -&gt; bool:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Async</span><span class="p">:</span> <span class="n">Makes</span> <span class="n">a</span> <span class="n">subscribe</span> <span class="n">request</span> <span class="n">to</span> <span class="n">the</span> <span class="n">chain</span><span class="o">.</span> <span class="n">Waits</span> <span class="k">for</span> <span class="n">subscription</span> <span class="n">inclusion</span> <span class="ow">or</span> <span class="n">returns</span> <span class="kc">False</span>
        <span class="n">Returns</span><span class="p">:</span>
            <span class="n">subscribed</span><span class="p">:</span> <span class="p">(</span><span class="nb">bool</span><span class="p">):</span> <span class="n">true</span> <span class="k">if</span> <span class="n">the</span> <span class="n">subscription</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">success</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-19'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-19'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">set_debug</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loop</span><span class="o">.</span><span class="n">run_until_complete</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">async_unsubscribe</span><span class="p">())</span>  

    <span class="k">async</span> <span class="k">def</span> <span class="nf">async_unsubscribe</span> <span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Async: Unsubscribes the local neuron from the chain.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        loop = asyncio.get_event_loop()</span>
<span class="sd">        loop.set_debug(enabled=True)</span>
<span class="sd">        return loop.run_until_complete(self.async_connect())</span>

<span class="sd">    async def async_connect(self) -&gt; bool:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Async</span><span class="p">:</span> <span class="n">Makes</span> <span class="ow">and</span> <span class="n">awaits</span> <span class="k">for</span> <span class="n">a</span> <span class="n">connection</span> <span class="n">to</span> <span class="n">the</span> <span class="n">chain</span><span class="o">.</span>
        <span class="n">Returns</span><span class="p">:</span>
            <span class="n">connected</span><span class="p">:</span> <span class="p">(</span><span class="nb">bool</span><span class="p">):</span> <span class="n">true</span> <span class="k">if</span> <span class="n">the</span> <span class="n">connection</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">success</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-20'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-20'>#</a>
      </div>
      <p>return self.incentive</p>
<pre><code>@property
def ranks(self) -&gt; torch.FloatTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns the ranks W^t * S
Returns
    ranks: (:obj:<code>torch.FloatTensor</code> of shape :obj:<code>(n)</code>):
        rank of each known neuron.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">set_debug</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">run_until_complete</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">async_emit</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">async_emit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Emits the passed weights to the chain. Waits for inclusion.</span>
<span class="sd">        Args:</span>
<span class="sd">            weights: (torch.FloatTensor): </span>
<span class="sd">                weights to set on chain.</span>
<span class="sd">        Return:</span>
<span class="sd">            included: (bool) true is the weights were set on chain.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        loop = asyncio.get_event_loop()</span>
<span class="sd">        loop.set_debug(enabled=True)</span>
<span class="sd">        return loop.run_until_complete(self.async_sync(weights))</span>

<span class="sd">    async def async_sync(self, weights: torch.FloatTensor) -&gt; torch.FloatTensor:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Async</span><span class="p">:</span> <span class="n">Synchronizes</span> <span class="n">the</span> <span class="n">local</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="k">with</span> <span class="n">the</span> <span class="n">chain</span> <span class="n">state</span> <span class="n">by</span> <span class="n">polling</span> <span class="n">the</span> <span class="n">chain</span><span class="o">.</span>
            <span class="n">Args</span><span class="p">:</span>
                <span class="n">weights</span><span class="p">:</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">):</span>
                    <span class="n">weights</span> <span class="n">to</span> <span class="nb">set</span> <span class="n">on</span> <span class="n">chain</span><span class="o">.</span>
            <span class="n">Returns</span><span class="p">:</span>
                <span class="n">weights</span><span class="p">:</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">):</span>
                    <span class="n">weights</span> <span class="n">on</span> <span class="n">chain</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-21'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-21'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">calls</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_block</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_block</span><span class="p">()</span>
        <span class="n">emits</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtensor_client</span><span class="o">.</span><span class="n">get_last_emit_data</span><span class="p">()</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">pubkey</span><span class="p">,</span> <span class="n">last_emit</span><span class="p">)</span> <span class="ow">in</span> <span class="n">emits</span><span class="p">:</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-22'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-22'>#</a>
      </div>
      <p>S.shape = [self.state.n]
W.shape = [self.state.n, self.state.n]
R.shape = [self.state.n]</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>                <span class="k">if</span> <span class="p">(</span><span class="n">current_block</span> <span class="o">-</span> <span class="n">last_emit</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">metagraph</span><span class="o">.</span><span class="n">stale_emit_filter</span><span class="p">:</span>
                    <span class="n">calls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_poll_pubkey</span><span class="p">(</span><span class="n">pubkey</span><span class="p">))</span>
        <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">calls</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_poll_pubkey</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pubkey</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Polls info info for a specfic public key.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        start_time = time.time()</span>
<span class="sd">        info = await self.subtensor_client.neurons(self.__keypair.public_key)</span>
<span class="sd">        while info == None:</span>
<span class="sd">            await asyncio.sleep(1)</span>
<span class="sd">            info = await self.subtensor_client.neurons(self.__keypair.public_key)</span>
<span class="sd">            if time.time() - start_time &gt; timeout:</span>
<span class="sd">                return False</span>
<span class="sd">        return True</span>

<span class="sd">    async def _are_set_on_chain(self, keys, vals) -&gt; bool:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Returns</span> <span class="n">true</span> <span class="k">if</span> <span class="n">the</span> <span class="n">passed</span> <span class="n">key</span> <span class="ow">and</span> <span class="n">vals</span> <span class="n">are</span> <span class="nb">set</span> <span class="n">on</span> <span class="n">chain</span><span class="o">.</span></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-23'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-23'>#</a>
      </div>
      <p>return self.ranks()</p>
<pre><code>@property
def W(self) -&gt; torch.FloatTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Full chain weight matrix for each neuron.
Returns
    W: (:obj:<code>torch.LongFloat</code> of shape :obj:<code>(n, n)</code>):
        w_ij of each neuron.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre>        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_are_set_on_chain</span><span class="p">(</span><span class="n">weight_keys</span><span class="p">,</span> <span class="n">weight_vals</span><span class="p">):</span>
            <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">timeout</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Timeout while waiting for emit inclusion.&#39;</span><span class="p">)</span>
                <span class="k">return</span> <span class="kc">False</span>
        <span class="n">chain_keys</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtensor_client</span><span class="o">.</span><span class="n">weight_keys</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__keypair</span><span class="o">.</span><span class="n">public_key</span><span class="p">)</span>
        <span class="n">chain_vals</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">subtensor_client</span><span class="o">.</span><span class="n">weight_vals</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__keypair</span><span class="o">.</span><span class="n">public_key</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Chain weights </span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">chain_keys</span><span class="p">,</span><span class="n">chain_vals</span><span class="p">)))</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_remove_noop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_keys</span><span class="p">,</span> <span class="n">weight_vals</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Removes weights and vals from the chain update which have not changed on chain.</span>
<span class="sd">        Returns:</span>
<span class="sd">            keys, vals:</span>
<span class="sd">                keys, vals with removed noops.</span>
<span class="sd">#DIVIDER</span>
<span class="sd">        as_list = weights.tolist()</span>
<span class="sd">        if len(as_list) != self.state.n:</span>
<span class="sd">            logger.error(&quot;Error trying to set weights on chain. Got length {}, but the length must match the number of neurons in self.state.n {}&quot;, len(as_lsit), self.state.n)</span>
<span class="sd">            return False</span>
<span class="sd">        sum_list = sum(as_list)</span>
<span class="sd">        epsilon = 0.001</span>
<span class="sd">        if abs(1.0 - sum_list) &gt; epsilon:</span>
<span class="sd">            logger.error(&quot;Error trying to set weights on chain. Got {} but sum of weights must equal 1&quot;, sum_list)</span>
<span class="sd">            return False</span>
<span class="sd">        min_list = min(as_list)</span>
<span class="sd">        if min_list &lt; 0.0:</span>
<span class="sd">            logger.error(&quot;Error trying to set weights on chain. Got min value {} but values must be in range [0,1]&quot;, min_list)</span>
<span class="sd">            return False</span>
<span class="sd">        max_list = max(as_list)</span>
<span class="sd">        if max_list &gt; 1.0:</span>
<span class="sd">            logger.error(&quot;Error trying to set weights on chain. Got max value {} but values must be in range [0,1]&quot;, max_list)</span>
<span class="sd">            return False</span>
<span class="sd">        return True</span>
<span class="sd">#DIVIDER</span>
<span class="sd">    def _convert_weights(self, weights: torch.FloatTensor) -&gt; Tuple[List[str], List[int]]:</span>
<span class="sd">        r&quot;&quot;&quot;</span> <span class="n">Converts</span> <span class="n">weights</span> <span class="n">into</span> <span class="n">integer</span> <span class="n">u32</span> <span class="n">representation</span><span class="o">.</span>
        <span class="n">Returns</span><span class="p">:</span>
            <span class="n">keys</span><span class="p">:</span> <span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
                <span class="n">List</span> <span class="n">of</span> <span class="n">pubkeys</span> <span class="n">associated</span> <span class="k">with</span> <span class="n">each</span> <span class="n">weight</span> <span class="kn">from</span> <span class="nn">vals.</span>
            <span class="n">vals</span><span class="p">:</span> <span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
                <span class="n">List</span> <span class="n">of</span> <span class="n">u32</span> <span class="n">integer</span> <span class="n">representations</span> <span class="n">of</span> <span class="n">floating</span> <span class="n">point</span> <span class="n">weights</span><span class="o">.</span>

</pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-24'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-24'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-25'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-25'>#</a>
      </div>
      <p>return self.state.neurons</p>
<pre><code>@property
def public_keys(self) -&gt; List[str]:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Return the ordered public keys for state neurons.
Returns
    public_keys: (:obj:<code>List[str]</code> of shape :obj:<code>(n)</code>):
        public keys of all graph neurons.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-26'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-26'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-27'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-27'>#</a>
      </div>
      <p>if self.state.n == 0:
    return torch.Tensor([])
else:
    w_0 = self.state.W[0,:]
    return w_0</p>
<pre><code>def uids_to_indices(self, uids: torch.Tensor):
</code></pre>
<p>r&rdquo;&ldquo;&rdquo;Return the indices of passed uids
Args:
    uids: (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(-1)</code>):
        UIDs for indices
Returns 
    indices: (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(-1)</code>):
        returned indices for passed uids.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-28'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-28'>#</a>
      </div>
      
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-29'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-29'>#</a>
      </div>
      <p>response = []
indices = self.uids_to_indices(uids)
for idx in indices.tolist():
    response.append(self.state.neurons[idx])
return response</p>
<pre><code>def chain_weights(self) -&gt; torch.FloatTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns your current weights from the chain.
Returns:
    weights: (torch.Tensor) weights on chain as torch tensor.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-30'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-30'>#</a>
      </div>
      <p>chain_keys = await self.subtensor_client.weight_keys(self.__keypair.public_key)
chain_vals = await self.subtensor_client.weight_vals(self.__keypair.public_key)
val_sum = sum(chain_vals)
retval = torch.zeros(self._n)
for key, val in list(zip(chain_keys, chain_vals)):
    idx = self._index_for_pubkey[key]
    if idx &gt;= self._n:
        continue
    else:
        retval[idx] = float(val) / float(val_sum)
return retval</p>
<pre><code>def block(self):
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Returns the current block on the chain.
Returns:
    block: (int) block number on chain.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-31'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-31'>#</a>
      </div>
      <p>return await self.subtensor_client.get_current_block()</p>
<pre><code>def subscribe(self, timeout) -&gt; bool:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Syncronous: Makes a subscribe request to the chain. Waits for subscription inclusion or returns False
Returns:
    subscribed: (bool): true if the subscription is a success.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-32'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-32'>#</a>
      </div>
      <p>await self.subtensor_client.subscribe(self._config.axon.external_ip, self._config.axon.port)
return await self._wait_for_subscription(timeout=12)</p>
<pre><code>def unsubscribe(self) -&gt; bool:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Syncronous: Unsubscribes the local neuron from the chain.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-33'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-33'>#</a>
      </div>
      <p>logger.info(&lsquo;Unsubscribe from chain endpoint&rsquo;)
await self.subtensor_client.unsubscribe()</p>
<pre><code>def connect(self) -&gt; bool:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Synchronous: Connects to the chain.
Returns:
    connected: (bool): true if the connection is a success.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-34'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-34'>#</a>
      </div>
      <p>self.subtensor_client.connect()
connected = await self.subtensor_client.is_connected()
return connected        </p>
<pre><code>def emit(self, weights: torch.FloatTensor):
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Emits the passed weights to the chain. Waits for inclusion.
Args:
    weights: (torch.FloatTensor): 
        weights to set on chain of length self.state.n</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-35'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-35'>#</a>
      </div>
      <p>logger.info(&lsquo;Emit -&gt; {}&rsquo;, weights)</p>
<h1>Check that weights meet chain requirements.</h1>
<h1>#TODO(const) check with current weights.</h1>
<p>if not self._check_weights(weights):
    logger.error(&lsquo;Weight emit failed with weight check.&rsquo;)
    return False</p>
<h1>Convert weights to integer represenation and get corresponding keys.</h1>
<p>keys, vals = self._convert_weights(weights)</p>
<h1>Remove unchanged vals.</h1>
<p>keys, vals = await self._remove_noop(keys, vals)
if len(keys) == 0:
    logger.error(&lsquo;Weight emit is a no-op.&rsquo;)
    return False</p>
<h1>Makes weight emission call.</h1>
<h1>TODO(const): make wait for inclusion work.</h1>
<p>try:
    await self.subtensor_client.set_weights(keys, vals, self.__keypair, wait_for_inclusion = False)
except Exception as e:
    logger.warning(&lsquo;Failed to emit weights with error {}, and weights {}&rsquo;, e, list(zip(keys, vals)))
    return False</p>
<h1>Checks that weight emission was included in a block after 12 seconds.</h1>
<p>if not await self._wait_for_emit_inclusion(keys, vals, timeout = 12):
    logger.error(&lsquo;Weight failed with non-inclusion after 12 seconds.&rsquo;)
    return False
return True</p>
<pre><code>def sync(self, weights: torch.FloatTensor) -&gt; torch.FloatTensor:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Synchronizes the local self.state with the chain state, sinking the trained weights and pulling 
info from other peers. Ensures the self.state is in accordance with the state on chain at this block.
    Args:
        weights: (torch.FloatTensor):
            weights to set on chain.
    Returns:
        weights: (torch.FloatTensor):
            weights on chain.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-36'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-36'>#</a>
      </div>
      <p>if weights != None:
    await self.async_emit(weights)
await self._sync_cache()
last_sync = await self.async_block()
self.state = TorchChainState.from_cache(self.cache)
self.state.block = last_sync
return self.weights</p>
<pre><code>async def _sync_cache(self):
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Async: Makes calls to chain updating local chain cache with newest info.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-37'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-37'>#</a>
      </div>
      <p>Make asyncronous calls to chain filling local state cache.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-38'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-38'>#</a>
      </div>
      <p>Filter based on stale emissions.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-39'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-39'>#</a>
      </div>
      <p>logger.info(&lsquo;poll: {} &lsquo;, pubkey)
try:
    stake = await self.subtensor_client.get_stake(pubkey)
    lastemit = await self.subtensor_client.get_last_emit_data(pubkey)
    info = await self.subtensor_client.neurons(pubkey)
    w_keys = await self.subtensor_client.weight_keys(pubkey)
    w_vals = await self.subtensor_client.weight_vals(pubkey)
    self.cache.add_or_update(pubkey = pubkey, ip = info[&lsquo;ip&rsquo;], port = info[&lsquo;port&rsquo;], lastemit = lastemit, stake = stake, w_keys = w_keys, w_vals = w_vals)
except Exception as e:
    logger.error(&ldquo;Exception occurred: {}&rdquo;.format(e))
    traceback.print_exc()</p>
<pre><code>async def _wait_for_subscription(self, timeout=12) -&gt; bool:
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Async: Waits for subscription info to appear on chain.
Returns:
    subscribed: (bool): true if info is set on chain after timeout.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-40'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-40'>#</a>
      </div>
      <p>cmap = {}
chain_keys = await self.subtensor_client.weight_keys(self.__keypair.public_key)
chain_vals = await self.subtensor_client.weight_vals(self.__keypair.public_key)
for key, val in list(zip(chain_keys, chain_vals)):
    cmap[key] = val
for key, val in list(zip(keys, vals)):
    if key not in cmap:
        return False
    if cmap[key] != val:
        return False 
return True</p>
<pre><code>async def _wait_for_emit_inclusion(self, weight_keys, weight_vals, timeout=12):
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Waits until timeout for the local keys and vals to be set on chain.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-41'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-41'>#</a>
      </div>
      <p>cmap = {}
chain_keys = await self.subtensor_client.weight_keys(self.__keypair.public_key)
chain_vals = await self.subtensor_client.weight_vals(self.__keypair.public_key)
for key, val in list(zip(chain_keys, chain_vals)):
    cmap[key] = val</p>
<p>ret_keys = []
ret_vals = []
for key, val in list(zip(weight_keys, weight_vals)):
    if key in cmap:
        if cmap[key] == val:
            continue 
    ret_keys.append(key)
    ret_vals.append(val)</p>
<p>return ret_keys, ret_vals          </p>
<pre><code>def _check_weights(self, weights: torch.Tensor):
</code></pre>
<p>r&rdquo;&ldquo;&rdquo; Checks that weights vector being set on chain meet requirements.
Returns:
    valid: (bool)
        True if the weight being set meet requirements to be set on chain.</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
  <div class='section' id='section-42'>
    <div class='docs'>
      <div class='octowrap'>
        <a class='octothorpe' href='#section-42'>#</a>
      </div>
      <h1>Convert floats to ints with precision.</h1>
<p>u32_int_max = 4294967295 # max u32 int value.
weight_pubkeys = []
weight_vals_as_ints = []
for i, val in enumerate(weights.tolist()):
    weight_pubkeys.append( self.cache.pubkey_for_index[i] ) # Gets the pubkey at this index.
    int_val = int(float(val) * int(u32_int_max)) # convert to int representation.
    weight_vals_as_ints.append(int_val) # int weights sum to u32_int_max.
return weight_pubkeys, weight_vals_as_ints</p>
    </div>
    <div class='code'>
      <div class="highlight"><pre></pre></div>
    </div>
  </div>
  <div class='clearall'></div>
</div>
</body>
